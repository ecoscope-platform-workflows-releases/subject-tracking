# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_max,
    dataframe_column_mean,
    dataframe_column_sum,
    dataframe_count,
)
from ecoscope_workflows_core.tasks.config import set_string_var, set_workflow_details
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range,
    set_time_range,
)
from ecoscope_workflows_core.tasks.groupby import set_groupers, split_groups
from ecoscope_workflows_core.tasks.io import persist_text, set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view,
    create_plot_widget_single_view,
    create_single_value_widget_single_view,
    gather_dashboard,
    merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped,
    any_is_empty_df,
    never,
)
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index,
    convert_column_values_to_string,
    convert_values_to_timezone,
    map_columns,
    map_values,
    sort_values,
    with_unit,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density,
    get_night_day_ratio,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_subjectgroup_observations
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations,
    relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polygon_layer,
    create_polyline_layer,
    draw_ecomap,
    draw_ecoplot,
    set_base_maps,
)
from ecoscope_workflows_ext_ecoscope.tasks.skip import all_geometry_are_none
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification,
    apply_color_map,
    classify_is_night,
)

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    get_timezone = (
        get_timezone_from_time_range.validate()
        .set_task_instance_id("get_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(time_range=time_range, **(params_dict.get("get_timezone") or {}))
        .call()
    )

    subject_obs = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_obs") or {}),
        )
        .call()
    )

    convert_to_user_timezone = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_to_user_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=subject_obs,
            timezone=get_timezone,
            columns=["fixtime"],
            **(params_dict.get("convert_to_user_timezone") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=convert_to_user_timezone,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__subject_subtype",
                "extra__subject__sex",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    day_night_labels = (
        classify_is_night.validate()
        .set_task_instance_id("day_night_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=subject_reloc, **(params_dict.get("day_night_labels") or {})
        )
        .call()
    )

    subject_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("subject_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=day_night_labels, **(params_dict.get("subject_traj") or {})
        )
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=subject_traj,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    rename_grouper_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_grouper_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__name": "subject_name",
                "extra__subject_subtype": "subject_subtype",
                "extra__sex": "subject_sex",
            },
            **(params_dict.get("rename_grouper_columns") or {}),
        )
        .call()
    )

    map_subject_sex = (
        map_values.validate()
        .set_task_instance_id("map_subject_sex")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_grouper_columns,
            column_name="subject_sex",
            value_map={"male": "male", "female": "female"},
            missing_values="replace",
            replacement="unknown",
            **(params_dict.get("map_subject_sex") or {}),
        )
        .call()
    )

    classify_traj_speed = (
        apply_classification.validate()
        .set_task_instance_id("classify_traj_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=map_subject_sex,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={
                "label_ranges": True,
                "label_decimals": 1,
                "label_suffix": " km/h",
            },
            **(params_dict.get("classify_traj_speed") or {}),
        )
        .call()
    )

    set_traj_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_traj_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Subject Group Trajectory Map",
            **(params_dict.get("set_traj_map_title") or {}),
        )
        .call()
    )

    set_td_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_td_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(var="Home Range Map", **(params_dict.get("set_td_map_title") or {}))
        .call()
    )

    set_night_day_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_night_day_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Subject Group Night/Day Map",
            **(params_dict.get("set_night_day_map_title") or {}),
        )
        .call()
    )

    set_nsd_chart_title = (
        set_string_var.validate()
        .set_task_instance_id("set_nsd_chart_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Net Square Displacement",
            **(params_dict.get("set_nsd_chart_title") or {}),
        )
        .call()
    )

    split_subject_traj_groups = (
        split_groups.validate()
        .set_task_instance_id("split_subject_traj_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=classify_traj_speed,
            groupers=groupers,
            **(params_dict.get("split_subject_traj_groups") or {}),
        )
        .call()
    )

    base_map_defs = (
        set_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    sort_traj_speed = (
        sort_values.validate()
        .set_task_instance_id("sort_traj_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="speed_kmhr",
            ascending=True,
            na_position="last",
            **(params_dict.get("sort_traj_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    colormap_traj_speed = (
        apply_color_map.validate()
        .set_task_instance_id("colormap_traj_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("colormap_traj_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_traj_speed)
    )

    rename_speed_display_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_speed_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "segment_start": "Start",
                "timespan_seconds": "Duration (s)",
                "speed_kmhr": "Speed (kph)",
                "extra__is_night": "Nighttime",
                "subject_name": "Subject Name",
                "subject_sex": "Subject Sex",
            },
            **(params_dict.get("rename_speed_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=colormap_traj_speed)
    )

    traj_map_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("traj_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "speed_bins_colormap"},
            legend={
                "label_column": "speed_bins",
                "color_column": "speed_bins_colormap",
            },
            tooltip_columns=[
                "Start",
                "Duration (s)",
                "Speed (kph)",
                "Nighttime",
                "Subject Name",
                "Subject Sex",
            ],
            **(params_dict.get("traj_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=rename_speed_display_columns)
    )

    traj_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("traj_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": "Speed",
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            title=None,
            max_zoom=20,
            widget_id=set_traj_map_title,
            **(params_dict.get("traj_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=traj_map_layers)
    )

    ecomap_html_urls = (
        persist_text.validate()
        .set_task_instance_id("ecomap_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("ecomap_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=traj_ecomap)
    )

    traj_map_widgets_single_views = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("traj_map_widgets_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_traj_map_title,
            **(params_dict.get("traj_map_widgets_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=ecomap_html_urls)
    )

    traj_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("traj_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=traj_map_widgets_single_views,
            **(params_dict.get("traj_grouped_map_widget") or {}),
        )
        .call()
    )

    sort_traj_night_day = (
        sort_values.validate()
        .set_task_instance_id("sort_traj_night_day")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="extra__is_night",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_traj_night_day") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    colormap_traj_night = (
        apply_color_map.validate()
        .set_task_instance_id("colormap_traj_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            colormap=["#292965", "#e7a553"],
            input_column_name="extra__is_night",
            output_column_name="is_night_colors",
            **(params_dict.get("colormap_traj_night") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_traj_night_day)
    )

    rename_nightday_display_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_nightday_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "subject_name": "Subject Name",
                "subject_subtype": "Subject Sex",
                "extra__is_night": "Nighttime",
            },
            **(params_dict.get("rename_nightday_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=colormap_traj_night)
    )

    traj_map_night_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("traj_map_night_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "is_night_colors"},
            legend={"labels": ["Night", "Day"], "colors": ["#292965", "#e7a553"]},
            tooltip_columns=["Subject Name", "Subject Sex", "Nighttime"],
            **(params_dict.get("traj_map_night_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=rename_nightday_display_columns)
    )

    traj_nightday_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("traj_nightday_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": "Day / Night Movement",
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            title=None,
            max_zoom=20,
            widget_id=set_night_day_map_title,
            **(params_dict.get("traj_nightday_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=traj_map_night_layers)
    )

    ecomap_nightday_html_urls = (
        persist_text.validate()
        .set_task_instance_id("ecomap_nightday_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("ecomap_nightday_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=traj_nightday_ecomap)
    )

    traj_map_nightday_widgets_sv = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("traj_map_nightday_widgets_sv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_night_day_map_title,
            **(params_dict.get("traj_map_nightday_widgets_sv") or {}),
        )
        .map(argnames=["view", "data"], argvalues=ecomap_nightday_html_urls)
    )

    traj_nightday_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("traj_nightday_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=traj_map_nightday_widgets_sv,
            **(params_dict.get("traj_nightday_grouped_map_widget") or {}),
        )
        .call()
    )

    mean_speed = (
        dataframe_column_mean.validate()
        .set_task_instance_id("mean_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="speed_kmhr", **(params_dict.get("mean_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    average_speed_converted = (
        with_unit.validate()
        .set_task_instance_id("average_speed_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="km/h",
            new_unit="km/h",
            **(params_dict.get("average_speed_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=mean_speed)
    )

    mean_speed_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("mean_speed_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Mean Speed",
            decimal_places=1,
            **(params_dict.get("mean_speed_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=average_speed_converted)
    )

    mean_speed_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("mean_speed_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=mean_speed_sv_widgets,
            **(params_dict.get("mean_speed_grouped_sv_widget") or {}),
        )
        .call()
    )

    max_speed = (
        dataframe_column_max.validate()
        .set_task_instance_id("max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="speed_kmhr", **(params_dict.get("max_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    max_speed_converted = (
        with_unit.validate()
        .set_task_instance_id("max_speed_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="km/h",
            new_unit="km/h",
            **(params_dict.get("max_speed_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=max_speed)
    )

    max_speed_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("max_speed_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Max Speed",
            decimal_places=1,
            **(params_dict.get("max_speed_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=max_speed_converted)
    )

    max_speed_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("max_speed_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=max_speed_sv_widgets,
            **(params_dict.get("max_speed_grouped_sv_widget") or {}),
        )
        .call()
    )

    num_location = (
        dataframe_count.validate()
        .set_task_instance_id("num_location")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("num_location") or {}))
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    num_location_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("num_location_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Number of Locations",
            decimal_places=1,
            **(params_dict.get("num_location_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=num_location)
    )

    num_location_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("num_location_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=num_location_sv_widgets,
            **(params_dict.get("num_location_grouped_sv_widget") or {}),
        )
        .call()
    )

    nightday_ratio = (
        get_night_day_ratio.validate()
        .set_task_instance_id("nightday_ratio")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("nightday_ratio") or {}))
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    nightday_ratio_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("nightday_ratio_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Night/Day Ratio",
            decimal_places=1,
            **(params_dict.get("nightday_ratio_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=nightday_ratio)
    )

    nightday_ratio_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("nightday_ratio_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=nightday_ratio_sv_widgets,
            **(params_dict.get("nightday_ratio_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_distance = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_distance")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="dist_meters", **(params_dict.get("total_distance") or {}))
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    total_dist_converted = (
        with_unit.validate()
        .set_task_instance_id("total_dist_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="m",
            new_unit="km",
            **(params_dict.get("total_dist_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=total_distance)
    )

    total_distance_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_distance_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Distance",
            decimal_places=1,
            **(params_dict.get("total_distance_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_dist_converted)
    )

    total_dist_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_dist_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_distance_sv_widgets,
            **(params_dict.get("total_dist_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_time = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_time")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="timespan_seconds", **(params_dict.get("total_time") or {})
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    total_time_converted = (
        with_unit.validate()
        .set_task_instance_id("total_time_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="s",
            new_unit="h",
            **(params_dict.get("total_time_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=total_time)
    )

    total_time_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_time_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Time",
            decimal_places=1,
            **(params_dict.get("total_time_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_time_converted)
    )

    total_time_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_time_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_time_sv_widgets,
            **(params_dict.get("total_time_grouped_sv_widget") or {}),
        )
        .call()
    )

    td = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("td")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 99.999],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("td") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_subject_traj_groups)
    )

    percentile_col_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("percentile_col_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["percentile"],
            **(params_dict.get("percentile_col_to_string") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=td)
    )

    td_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("td_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn_r",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=percentile_col_to_string)
    )

    td_map_layer = (
        create_polygon_layer.validate()
        .set_task_instance_id("td_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "percentile_colormap",
                "opacity": 0.7,
                "get_line_width": 0,
            },
            legend={
                "label_column": "percentile",
                "label_suffix": " %",
                "color_column": "percentile_colormap",
                "sort": "ascending",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("td_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=td_colormap)
    )

    td_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("td_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": "Time Spent",
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            title=None,
            max_zoom=20,
            widget_id=set_td_map_title,
            **(params_dict.get("td_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=td_map_layer)
    )

    td_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("td_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("td_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=td_ecomap)
    )

    td_map_widget = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("td_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(title=set_td_map_title, **(params_dict.get("td_map_widget") or {}))
        .map(argnames=["view", "data"], argvalues=td_ecomap_html_url)
    )

    td_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("td_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=td_map_widget, **(params_dict.get("td_grouped_map_widget") or {})
        )
        .call()
    )

    nsd_rename_display_columns = (
        map_columns.validate()
        .set_task_instance_id("nsd_rename_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={"segment_start": "Time", "nsd": "NSD (m²)"},
            **(params_dict.get("nsd_rename_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    nsd_chart = (
        draw_ecoplot.validate()
        .set_task_instance_id("nsd_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            group_by="subject_name",
            ecoplot_configs=[
                {
                    "x_col": "Time",
                    "y_col": "NSD (m²)",
                    "plot_style": {"xperiodalignment": None},
                    "color_column": None,
                }
            ],
            tickformat="%b-%Y",
            widget_id=set_nsd_chart_title,
            **(params_dict.get("nsd_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=nsd_rename_display_columns)
    )

    nsd_chart_html_url = (
        persist_text.validate()
        .set_task_instance_id("nsd_chart_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("nsd_chart_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=nsd_chart)
    )

    nsd_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("nsd_chart_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_nsd_chart_title, **(params_dict.get("nsd_chart_widget") or {})
        )
        .map(argnames=["view", "data"], argvalues=nsd_chart_html_url)
    )

    grouped_nsd_chart_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_nsd_chart_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=nsd_chart_widget,
            **(params_dict.get("grouped_nsd_chart_widget_merge") or {}),
        )
        .call()
    )

    subject_tracking_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("subject_tracking_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                traj_grouped_map_widget,
                mean_speed_grouped_sv_widget,
                max_speed_grouped_sv_widget,
                num_location_grouped_sv_widget,
                nightday_ratio_grouped_sv_widget,
                total_dist_grouped_sv_widget,
                total_time_grouped_sv_widget,
                td_grouped_map_widget,
                traj_nightday_grouped_map_widget,
                grouped_nsd_chart_widget_merge,
            ],
            groupers=groupers,
            time_range=time_range,
            **(params_dict.get("subject_tracking_dashboard") or {}),
        )
        .call()
    )

    return subject_tracking_dashboard
